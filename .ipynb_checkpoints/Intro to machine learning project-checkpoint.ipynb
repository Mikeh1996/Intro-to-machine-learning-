{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning\n",
    "## Inspiration\n",
    "This is my first experience using machine learning, and I’m thrilled to be starting this journey. After working through statistical data analysis, I’m now excited to apply my data preprocessing skills to build my first classification model. I look forward to diving deeper into how machine learning algorithms work and eventually creating my own models from scratch. This project is the first step toward that goal.\n",
    "\n",
    "## Project Goal\n",
    "Build a machine learning classification model that recommends the correct mobile plan, Smart or Ultra for Megaline subscribers based on their monthly usage behavior. The goal is to develop a model with an accuracy of at least 0.75. Since the data is already preprocessed, the focus will be on model selection, tuning, and evaluation.\n",
    "\n",
    "## Project Plan\n",
    "### Analyze the Data\n",
    "\n",
    "* Open the file: users_behavior.csv\n",
    "\n",
    "* Explore the structure, features, and basic statistics to get a sense of the data.\n",
    "\n",
    "### Split the Data\n",
    "\n",
    "* Divide the dataset into:\n",
    "\n",
    "    * Training set\n",
    "\n",
    "    * Validation set\n",
    "\n",
    "    * Test set\n",
    "\n",
    "* Justify the proportions used for each split.\n",
    "\n",
    "### Model Selection and Tuning\n",
    "\n",
    "* Train multiple classification models (Decision Tree, Random Forest, Logistic Regression).\n",
    "\n",
    "* Tune hyperparameters to improve performance.\n",
    "\n",
    "* Compare models based on validation accuracy.\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "* Test the best performing model using the test dataset.\n",
    "\n",
    "* Check if the final accuracy meets or exceeds the 0.75 threshold.\n",
    "\n",
    "### Sanity Check\n",
    "\n",
    "* Analyze edge cases and misclassifications.\n",
    "\n",
    "* Evaluate if the model’s predictions make logical sense.\n",
    "\n",
    "* Identify any patterns or anomalies that require further review.\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "* Clearly describe:\n",
    "\n",
    "    * What models were used\n",
    "\n",
    "    * How they performed\n",
    "\n",
    "    * What tuning steps were effective\n",
    "\n",
    "    * Final test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_behavior = pd.read_csv('users_behavior.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to look over dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(df):\n",
    "    display(df.head())\n",
    "    print(' ')\n",
    "    print(df.info())\n",
    "    print('---------------------------')\n",
    "    print(' ')\n",
    "    print('Potential Duplicates')\n",
    "    print(' ')\n",
    "    print(df.duplicated().sum())\n",
    "    print('---------------------------')\n",
    "    print(' ')\n",
    "    print('Potential Missing Values')\n",
    "    print(' ')\n",
    "    print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>311.90</td>\n",
       "      <td>83.0</td>\n",
       "      <td>19915.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.0</td>\n",
       "      <td>516.75</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22696.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.0</td>\n",
       "      <td>467.66</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21060.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106.0</td>\n",
       "      <td>745.53</td>\n",
       "      <td>81.0</td>\n",
       "      <td>8437.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.0</td>\n",
       "      <td>418.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14502.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   calls  minutes  messages   mb_used  is_ultra\n",
       "0   40.0   311.90      83.0  19915.42         0\n",
       "1   85.0   516.75      56.0  22696.96         0\n",
       "2   77.0   467.66      86.0  21060.45         0\n",
       "3  106.0   745.53      81.0   8437.39         1\n",
       "4   66.0   418.74       1.0  14502.75         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n",
      "None\n",
      "---------------------------\n",
      " \n",
      "Potential Duplicates\n",
      " \n",
      "0\n",
      "---------------------------\n",
      " \n",
      "Potential Missing Values\n",
      " \n",
      "calls       0\n",
      "minutes     0\n",
      "messages    0\n",
      "mb_used     0\n",
      "is_ultra    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "analyze(user_behavior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 5 columns with 3,214 entries, the dataset is clean with no duplicates, or missing values. Columns calls and messages look like they should be int data type.\n",
    "\n",
    "* Corrections:\n",
    "    * Investigate if calls and messages should be int data type, if so change to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calls       0\n",
       "messages    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(user_behavior[['calls', 'messages']] % 1 != 0).sum() # Do calls and messages need to be float? No."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_behavior[['calls','messages']] = user_behavior[['calls', 'messages']].astype(int) # Changing columns to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   int64  \n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   int64  \n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(2), int64(3)\n",
      "memory usage: 125.7 KB\n"
     ]
    }
   ],
   "source": [
    "user_behavior.info() # Confirming changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into how many users are ultra to see if there will be imbalance, I want to make sure the training and validation sets have enough data so both classes are represented fairly in both splits (train + validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_behavior['is_ultra'].isin([1]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 985 users having ultra (22%) and 2,229 users having smart (78%), the plans are imbalanced but it's not extreme. I will use the stratify argument in train_split_test to ensure the splits will not be imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the split I will be doing a 2 way split, 60% train / 40% validation for these reasons:\n",
    "* Hyperparameter tuning for training multiple models.\n",
    "* A larger validation set will give a more reliable performance signal when comparing models.\n",
    "* The dataset isn't massive (3,200 rows) so there will still be enough training data (1,920 rows) while having a stronger validation base (1,280 rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = user_behavior.drop('is_ultra', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = user_behavior['is_ultra']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing split test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_valid, target_train, target_valid = train_test_split(features, target, test_size = 0.4, \n",
    "random_state = 12345, stratify = target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [] # empty list where results will go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning Hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in range(2, 7): # keeping max_depth around 4\n",
    "    for min_leaf in [1, 5, 10]: # training examples required for leaf node creation\n",
    "        for min_split in [2, 5, 10]: # amount of samples required for a split\n",
    "            model = DecisionTreeClassifier(\n",
    "                max_depth = depth,\n",
    "                min_samples_leaf = min_leaf,\n",
    "                min_samples_split = min_split,\n",
    "                random_state = 12345\n",
    "            )\n",
    "            model.fit(features_train, target_train) # training model variations\n",
    "            \n",
    "            # getting predictions\n",
    "            train_preds = model.predict(features_train)\n",
    "            valid_preds = model.predict(features_valid)\n",
    "            \n",
    "            # calculating accuracy\n",
    "            train_acc = accuracy_score(target_train, train_preds)\n",
    "            valid_acc = accuracy_score(target_valid, valid_preds)\n",
    "            \n",
    "            # saving results\n",
    "            results.append({\n",
    "                'max_depth': depth,\n",
    "                'min_samples_leaf': min_leaf,\n",
    "                'min_samples_split': min_split,\n",
    "                'train_accuracy': train_acc,\n",
    "                'valid_accuracy': valid_acc,\n",
    "                'gap': train_acc - valid_acc # spoting potential overfitting\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning results into dataframe to see results better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dtc = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dtc = results_dtc.sort_values(by = 'valid_accuracy', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.821577</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>0.021421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.821577</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>0.021421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.821577</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>0.021421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.813797</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0.015196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.812759</td>\n",
       "      <td>0.797823</td>\n",
       "      <td>0.014937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.811722</td>\n",
       "      <td>0.795490</td>\n",
       "      <td>0.016232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.811203</td>\n",
       "      <td>0.793935</td>\n",
       "      <td>0.017269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.811203</td>\n",
       "      <td>0.793935</td>\n",
       "      <td>0.017269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.811203</td>\n",
       "      <td>0.793935</td>\n",
       "      <td>0.017269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.811722</td>\n",
       "      <td>0.791602</td>\n",
       "      <td>0.020120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.811722</td>\n",
       "      <td>0.791602</td>\n",
       "      <td>0.020120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.811722</td>\n",
       "      <td>0.791602</td>\n",
       "      <td>0.020120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.825726</td>\n",
       "      <td>0.789269</td>\n",
       "      <td>0.036457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.823133</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.034641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.828320</td>\n",
       "      <td>0.786936</td>\n",
       "      <td>0.041383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.820021</td>\n",
       "      <td>0.780715</td>\n",
       "      <td>0.039305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.820021</td>\n",
       "      <td>0.780715</td>\n",
       "      <td>0.039305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.820021</td>\n",
       "      <td>0.780715</td>\n",
       "      <td>0.039305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.796162</td>\n",
       "      <td>0.771384</td>\n",
       "      <td>0.024778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.796162</td>\n",
       "      <td>0.771384</td>\n",
       "      <td>0.024778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.796162</td>\n",
       "      <td>0.771384</td>\n",
       "      <td>0.024778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.005327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.005327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.005327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.005327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.005327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.005327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.005327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.005327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.005327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.765941</td>\n",
       "      <td>0.031258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.765941</td>\n",
       "      <td>0.031258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.765941</td>\n",
       "      <td>0.031258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.764386</td>\n",
       "      <td>0.032813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.764386</td>\n",
       "      <td>0.032813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.764386</td>\n",
       "      <td>0.032813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_samples_leaf  min_samples_split  train_accuracy  \\\n",
       "44          6                10                 10        0.821577   \n",
       "43          6                10                  5        0.821577   \n",
       "42          6                10                  2        0.821577   \n",
       "27          5                 1                  2        0.813797   \n",
       "28          5                 1                  5        0.812759   \n",
       "29          5                 1                 10        0.811722   \n",
       "33          5                10                  2        0.811203   \n",
       "34          5                10                  5        0.811203   \n",
       "35          5                10                 10        0.811203   \n",
       "32          5                 5                 10        0.811722   \n",
       "31          5                 5                  5        0.811722   \n",
       "30          5                 5                  2        0.811722   \n",
       "37          6                 1                  5        0.825726   \n",
       "16          3                10                  5        0.790975   \n",
       "38          6                 1                 10        0.823133   \n",
       "17          3                10                 10        0.790975   \n",
       "15          3                10                  2        0.790975   \n",
       "13          3                 5                  5        0.790975   \n",
       "12          3                 5                  2        0.790975   \n",
       "11          3                 1                 10        0.790975   \n",
       "10          3                 1                  5        0.790975   \n",
       "9           3                 1                  2        0.790975   \n",
       "14          3                 5                 10        0.790975   \n",
       "36          6                 1                  2        0.828320   \n",
       "40          6                 5                  5        0.820021   \n",
       "41          6                 5                 10        0.820021   \n",
       "39          6                 5                  2        0.820021   \n",
       "24          4                10                  2        0.796162   \n",
       "25          4                10                  5        0.796162   \n",
       "26          4                10                 10        0.796162   \n",
       "0           2                 1                  2        0.775934   \n",
       "1           2                 1                  5        0.775934   \n",
       "8           2                10                 10        0.775934   \n",
       "7           2                10                  5        0.775934   \n",
       "6           2                10                  2        0.775934   \n",
       "5           2                 5                 10        0.775934   \n",
       "4           2                 5                  5        0.775934   \n",
       "3           2                 5                  2        0.775934   \n",
       "2           2                 1                 10        0.775934   \n",
       "23          4                 5                 10        0.797199   \n",
       "21          4                 5                  2        0.797199   \n",
       "22          4                 5                  5        0.797199   \n",
       "20          4                 1                 10        0.797199   \n",
       "19          4                 1                  5        0.797199   \n",
       "18          4                 1                  2        0.797199   \n",
       "\n",
       "    valid_accuracy       gap  \n",
       "44        0.800156  0.021421  \n",
       "43        0.800156  0.021421  \n",
       "42        0.800156  0.021421  \n",
       "27        0.798600  0.015196  \n",
       "28        0.797823  0.014937  \n",
       "29        0.795490  0.016232  \n",
       "33        0.793935  0.017269  \n",
       "34        0.793935  0.017269  \n",
       "35        0.793935  0.017269  \n",
       "32        0.791602  0.020120  \n",
       "31        0.791602  0.020120  \n",
       "30        0.791602  0.020120  \n",
       "37        0.789269  0.036457  \n",
       "16        0.788491  0.002484  \n",
       "38        0.788491  0.034641  \n",
       "17        0.788491  0.002484  \n",
       "15        0.788491  0.002484  \n",
       "13        0.788491  0.002484  \n",
       "12        0.788491  0.002484  \n",
       "11        0.788491  0.002484  \n",
       "10        0.788491  0.002484  \n",
       "9         0.788491  0.002484  \n",
       "14        0.788491  0.002484  \n",
       "36        0.786936  0.041383  \n",
       "40        0.780715  0.039305  \n",
       "41        0.780715  0.039305  \n",
       "39        0.780715  0.039305  \n",
       "24        0.771384  0.024778  \n",
       "25        0.771384  0.024778  \n",
       "26        0.771384  0.024778  \n",
       "0         0.770607  0.005327  \n",
       "1         0.770607  0.005327  \n",
       "8         0.770607  0.005327  \n",
       "7         0.770607  0.005327  \n",
       "6         0.770607  0.005327  \n",
       "5         0.770607  0.005327  \n",
       "4         0.770607  0.005327  \n",
       "3         0.770607  0.005327  \n",
       "2         0.770607  0.005327  \n",
       "23        0.765941  0.031258  \n",
       "21        0.765941  0.031258  \n",
       "22        0.765941  0.031258  \n",
       "20        0.764386  0.032813  \n",
       "19        0.764386  0.032813  \n",
       "18        0.764386  0.032813  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dtc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding best decision classification tree model by smallest gap with highest validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = results_dtc[(results_dtc['valid_accuracy'] >= 0.75)\n",
    "& (results_dtc['gap'] <= 0.005)].sort_values(by = 'valid_accuracy', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.790975</td>\n",
       "      <td>0.788491</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_samples_leaf  min_samples_split  train_accuracy  \\\n",
       "16          3                10                  5        0.790975   \n",
       "17          3                10                 10        0.790975   \n",
       "15          3                10                  2        0.790975   \n",
       "13          3                 5                  5        0.790975   \n",
       "12          3                 5                  2        0.790975   \n",
       "11          3                 1                 10        0.790975   \n",
       "10          3                 1                  5        0.790975   \n",
       "9           3                 1                  2        0.790975   \n",
       "14          3                 5                 10        0.790975   \n",
       "\n",
       "    valid_accuracy       gap  \n",
       "16        0.788491  0.002484  \n",
       "17        0.788491  0.002484  \n",
       "15        0.788491  0.002484  \n",
       "13        0.788491  0.002484  \n",
       "12        0.788491  0.002484  \n",
       "11        0.788491  0.002484  \n",
       "10        0.788491  0.002484  \n",
       "9         0.788491  0.002484  \n",
       "14        0.788491  0.002484  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Classification Tree Results\n",
    "For Decision Classification Tree, row 13 was chosen for these reasons:\n",
    "\n",
    "* min_samples_leaf = 5\n",
    "    This value strikes a balance:\n",
    "\n",
    "    * Not too low (1 = overfitting risk)\n",
    "\n",
    "    * Not too high (10 = underfitting risk)\n",
    "        It ensures leaf nodes aren’t too specific or too general.\n",
    "\n",
    "* min_samples_split = 5\n",
    "    This allows the tree to split on moderate-sized groups.\n",
    "    It helps capture meaningful patterns while still filtering out noise.\n",
    "\n",
    "* Balanced complexity\n",
    "    These values are in a sensible middle ground, offering the same accuracy as other configurations with no performance loss but less risk of overfitting or underfitting.\n",
    "\n",
    "* Interpretability\n",
    "    With a controlled max_depth and moderate split/leaf constraints, this model remains simpler to interpret and explain. If deeper splits are ever allowed later, it will require fewer samples to justify them keeping the structure understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf = [] # empty list where results will go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning hyperparameters for random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10, 51, 10):\n",
    "    for depth in [3, 5, 7]:\n",
    "        for leaf in [1, 5]:\n",
    "            for split in [2, 5]:\n",
    "                model_rf = RandomForestClassifier(\n",
    "                    n_estimators = n,\n",
    "                    max_depth = depth,\n",
    "                    min_samples_leaf = leaf,\n",
    "                    min_samples_split = split,\n",
    "                    random_state = 12345\n",
    "                )\n",
    "                model_rf.fit(features_train, target_train)\n",
    "                \n",
    "                # model predictions\n",
    "                train_preds = model_rf.predict(features_train) \n",
    "                valid_preds = model_rf.predict(features_valid) \n",
    "                \n",
    "                # model accuracy scores\n",
    "                train_acc = accuracy_score(target_train, train_preds)\n",
    "                valid_acc = accuracy_score(target_valid, valid_preds)\n",
    "                \n",
    "                results_rf.append({\n",
    "                    'n_estimators': n,\n",
    "                    'max_depth': depth,\n",
    "                    'min_samples_leaf': leaf,\n",
    "                    'min_samples_split': split,\n",
    "                    'train_accuracy': train_acc,\n",
    "                    'valid_accuracy': valid_acc,\n",
    "                    'gap': train_acc - valid_acc\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning results into dataframe to see results better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf_df = pd.DataFrame(results_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf_df = results_rf_df.sort_values(by = 'valid_accuracy', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.848548</td>\n",
       "      <td>0.814152</td>\n",
       "      <td>0.034395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.850104</td>\n",
       "      <td>0.813375</td>\n",
       "      <td>0.036729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.855290</td>\n",
       "      <td>0.812597</td>\n",
       "      <td>0.042693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.847510</td>\n",
       "      <td>0.812597</td>\n",
       "      <td>0.034913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.856328</td>\n",
       "      <td>0.811820</td>\n",
       "      <td>0.044508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.852178</td>\n",
       "      <td>0.811042</td>\n",
       "      <td>0.041136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.852697</td>\n",
       "      <td>0.811042</td>\n",
       "      <td>0.041655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.837656</td>\n",
       "      <td>0.810264</td>\n",
       "      <td>0.027391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.837137</td>\n",
       "      <td>0.810264</td>\n",
       "      <td>0.026873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.857884</td>\n",
       "      <td>0.810264</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.837137</td>\n",
       "      <td>0.810264</td>\n",
       "      <td>0.026873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.837656</td>\n",
       "      <td>0.810264</td>\n",
       "      <td>0.027391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.821058</td>\n",
       "      <td>0.810264</td>\n",
       "      <td>0.010794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.817946</td>\n",
       "      <td>0.810264</td>\n",
       "      <td>0.007682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.817946</td>\n",
       "      <td>0.810264</td>\n",
       "      <td>0.007682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.838174</td>\n",
       "      <td>0.809487</td>\n",
       "      <td>0.028687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.838174</td>\n",
       "      <td>0.809487</td>\n",
       "      <td>0.028687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.837656</td>\n",
       "      <td>0.808709</td>\n",
       "      <td>0.028946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.837656</td>\n",
       "      <td>0.808709</td>\n",
       "      <td>0.028946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.820021</td>\n",
       "      <td>0.808709</td>\n",
       "      <td>0.011312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.818465</td>\n",
       "      <td>0.806376</td>\n",
       "      <td>0.012088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.817427</td>\n",
       "      <td>0.806376</td>\n",
       "      <td>0.011051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.817427</td>\n",
       "      <td>0.806376</td>\n",
       "      <td>0.011051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.821577</td>\n",
       "      <td>0.806376</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.818465</td>\n",
       "      <td>0.806376</td>\n",
       "      <td>0.012088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.823133</td>\n",
       "      <td>0.805599</td>\n",
       "      <td>0.017534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.820539</td>\n",
       "      <td>0.805599</td>\n",
       "      <td>0.014941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.855809</td>\n",
       "      <td>0.805599</td>\n",
       "      <td>0.050210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.817946</td>\n",
       "      <td>0.804821</td>\n",
       "      <td>0.013125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.817946</td>\n",
       "      <td>0.804821</td>\n",
       "      <td>0.013125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833506</td>\n",
       "      <td>0.804044</td>\n",
       "      <td>0.029463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.833506</td>\n",
       "      <td>0.804044</td>\n",
       "      <td>0.029463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825726</td>\n",
       "      <td>0.803266</td>\n",
       "      <td>0.022460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.822095</td>\n",
       "      <td>0.803266</td>\n",
       "      <td>0.018829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.796680</td>\n",
       "      <td>0.802488</td>\n",
       "      <td>-0.005808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.821058</td>\n",
       "      <td>0.802488</td>\n",
       "      <td>0.018570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.796680</td>\n",
       "      <td>0.802488</td>\n",
       "      <td>-0.005808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.850622</td>\n",
       "      <td>0.802488</td>\n",
       "      <td>0.048134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797718</td>\n",
       "      <td>0.801711</td>\n",
       "      <td>-0.003993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797718</td>\n",
       "      <td>0.801711</td>\n",
       "      <td>-0.003993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800933</td>\n",
       "      <td>-0.002697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800933</td>\n",
       "      <td>-0.002697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800933</td>\n",
       "      <td>-0.002697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800933</td>\n",
       "      <td>-0.002697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797718</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.002438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.001919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.001919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.001919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.001919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797718</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.002438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.799378</td>\n",
       "      <td>-0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.799378</td>\n",
       "      <td>-0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.822614</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0.024014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.821058</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0.022458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.811203</td>\n",
       "      <td>0.793157</td>\n",
       "      <td>0.018046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.811203</td>\n",
       "      <td>0.793157</td>\n",
       "      <td>0.018046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  min_samples_leaf  min_samples_split  \\\n",
       "57            50          7                 1                  5   \n",
       "45            40          7                 1                  5   \n",
       "44            40          7                 1                  2   \n",
       "9             10          7                 1                  5   \n",
       "56            50          7                 1                  2   \n",
       "21            20          7                 1                  5   \n",
       "33            30          7                 1                  5   \n",
       "46            40          7                 5                  2   \n",
       "22            20          7                 5                  2   \n",
       "32            30          7                 1                  2   \n",
       "23            20          7                 5                  5   \n",
       "47            40          7                 5                  5   \n",
       "52            50          5                 1                  2   \n",
       "54            50          5                 5                  2   \n",
       "55            50          5                 5                  5   \n",
       "58            50          7                 5                  2   \n",
       "59            50          7                 5                  5   \n",
       "35            30          7                 5                  5   \n",
       "34            30          7                 5                  2   \n",
       "53            50          5                 1                  5   \n",
       "31            30          5                 5                  5   \n",
       "43            40          5                 5                  5   \n",
       "42            40          5                 5                  2   \n",
       "40            40          5                 1                  2   \n",
       "30            30          5                 5                  2   \n",
       "28            30          5                 1                  2   \n",
       "29            30          5                 1                  5   \n",
       "20            20          7                 1                  2   \n",
       "18            20          5                 5                  2   \n",
       "19            20          5                 5                  5   \n",
       "11            10          7                 5                  5   \n",
       "10            10          7                 5                  2   \n",
       "16            20          5                 1                  2   \n",
       "17            20          5                 1                  5   \n",
       "2             10          3                 5                  2   \n",
       "41            40          5                 1                  5   \n",
       "3             10          3                 5                  5   \n",
       "8             10          7                 1                  2   \n",
       "0             10          3                 1                  2   \n",
       "1             10          3                 1                  5   \n",
       "12            20          3                 1                  2   \n",
       "14            20          3                 5                  2   \n",
       "13            20          3                 1                  5   \n",
       "15            20          3                 5                  5   \n",
       "26            30          3                 5                  2   \n",
       "37            40          3                 1                  5   \n",
       "48            50          3                 1                  2   \n",
       "49            50          3                 1                  5   \n",
       "50            50          3                 5                  2   \n",
       "51            50          3                 5                  5   \n",
       "36            40          3                 1                  2   \n",
       "24            30          3                 1                  2   \n",
       "27            30          3                 5                  5   \n",
       "25            30          3                 1                  5   \n",
       "39            40          3                 5                  5   \n",
       "38            40          3                 5                  2   \n",
       "5             10          5                 1                  5   \n",
       "4             10          5                 1                  2   \n",
       "7             10          5                 5                  5   \n",
       "6             10          5                 5                  2   \n",
       "\n",
       "    train_accuracy  valid_accuracy       gap  \n",
       "57        0.848548        0.814152  0.034395  \n",
       "45        0.850104        0.813375  0.036729  \n",
       "44        0.855290        0.812597  0.042693  \n",
       "9         0.847510        0.812597  0.034913  \n",
       "56        0.856328        0.811820  0.044508  \n",
       "21        0.852178        0.811042  0.041136  \n",
       "33        0.852697        0.811042  0.041655  \n",
       "46        0.837656        0.810264  0.027391  \n",
       "22        0.837137        0.810264  0.026873  \n",
       "32        0.857884        0.810264  0.047619  \n",
       "23        0.837137        0.810264  0.026873  \n",
       "47        0.837656        0.810264  0.027391  \n",
       "52        0.821058        0.810264  0.010794  \n",
       "54        0.817946        0.810264  0.007682  \n",
       "55        0.817946        0.810264  0.007682  \n",
       "58        0.838174        0.809487  0.028687  \n",
       "59        0.838174        0.809487  0.028687  \n",
       "35        0.837656        0.808709  0.028946  \n",
       "34        0.837656        0.808709  0.028946  \n",
       "53        0.820021        0.808709  0.011312  \n",
       "31        0.818465        0.806376  0.012088  \n",
       "43        0.817427        0.806376  0.011051  \n",
       "42        0.817427        0.806376  0.011051  \n",
       "40        0.821577        0.806376  0.015200  \n",
       "30        0.818465        0.806376  0.012088  \n",
       "28        0.823133        0.805599  0.017534  \n",
       "29        0.820539        0.805599  0.014941  \n",
       "20        0.855809        0.805599  0.050210  \n",
       "18        0.817946        0.804821  0.013125  \n",
       "19        0.817946        0.804821  0.013125  \n",
       "11        0.833506        0.804044  0.029463  \n",
       "10        0.833506        0.804044  0.029463  \n",
       "16        0.825726        0.803266  0.022460  \n",
       "17        0.822095        0.803266  0.018829  \n",
       "2         0.796680        0.802488 -0.005808  \n",
       "41        0.821058        0.802488  0.018570  \n",
       "3         0.796680        0.802488 -0.005808  \n",
       "8         0.850622        0.802488  0.048134  \n",
       "0         0.797718        0.801711 -0.003993  \n",
       "1         0.797718        0.801711 -0.003993  \n",
       "12        0.798237        0.800933 -0.002697  \n",
       "14        0.798237        0.800933 -0.002697  \n",
       "13        0.798237        0.800933 -0.002697  \n",
       "15        0.798237        0.800933 -0.002697  \n",
       "26        0.797199        0.800156 -0.002956  \n",
       "37        0.797718        0.800156 -0.002438  \n",
       "48        0.798237        0.800156 -0.001919  \n",
       "49        0.798237        0.800156 -0.001919  \n",
       "50        0.798237        0.800156 -0.001919  \n",
       "51        0.798237        0.800156 -0.001919  \n",
       "36        0.797718        0.800156 -0.002438  \n",
       "24        0.797199        0.800156 -0.002956  \n",
       "27        0.797199        0.800156 -0.002956  \n",
       "25        0.797199        0.800156 -0.002956  \n",
       "39        0.798237        0.799378 -0.001141  \n",
       "38        0.798237        0.799378 -0.001141  \n",
       "5         0.822614        0.798600  0.024014  \n",
       "4         0.821058        0.798600  0.022458  \n",
       "7         0.811203        0.793157  0.018046  \n",
       "6         0.811203        0.793157  0.018046  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_rf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding top models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models_rf = results_rf_df[(results_rf_df['valid_accuracy'] >= 0.75) \n",
    "& (results_rf_df['gap'] <= 0.005)].sort_values(by = 'valid_accuracy', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.796680</td>\n",
       "      <td>0.802488</td>\n",
       "      <td>-0.005808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.796680</td>\n",
       "      <td>0.802488</td>\n",
       "      <td>-0.005808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797718</td>\n",
       "      <td>0.801711</td>\n",
       "      <td>-0.003993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797718</td>\n",
       "      <td>0.801711</td>\n",
       "      <td>-0.003993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800933</td>\n",
       "      <td>-0.002697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800933</td>\n",
       "      <td>-0.002697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800933</td>\n",
       "      <td>-0.002697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800933</td>\n",
       "      <td>-0.002697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.001919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797718</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.002438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.001919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.001919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.001919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797718</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.002438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797199</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>-0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.799378</td>\n",
       "      <td>-0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.798237</td>\n",
       "      <td>0.799378</td>\n",
       "      <td>-0.001141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  min_samples_leaf  min_samples_split  \\\n",
       "2             10          3                 5                  2   \n",
       "3             10          3                 5                  5   \n",
       "0             10          3                 1                  2   \n",
       "1             10          3                 1                  5   \n",
       "12            20          3                 1                  2   \n",
       "14            20          3                 5                  2   \n",
       "13            20          3                 1                  5   \n",
       "15            20          3                 5                  5   \n",
       "51            50          3                 5                  5   \n",
       "25            30          3                 1                  5   \n",
       "27            30          3                 5                  5   \n",
       "24            30          3                 1                  2   \n",
       "36            40          3                 1                  2   \n",
       "48            50          3                 1                  2   \n",
       "50            50          3                 5                  2   \n",
       "49            50          3                 1                  5   \n",
       "37            40          3                 1                  5   \n",
       "26            30          3                 5                  2   \n",
       "39            40          3                 5                  5   \n",
       "38            40          3                 5                  2   \n",
       "\n",
       "    train_accuracy  valid_accuracy       gap  \n",
       "2         0.796680        0.802488 -0.005808  \n",
       "3         0.796680        0.802488 -0.005808  \n",
       "0         0.797718        0.801711 -0.003993  \n",
       "1         0.797718        0.801711 -0.003993  \n",
       "12        0.798237        0.800933 -0.002697  \n",
       "14        0.798237        0.800933 -0.002697  \n",
       "13        0.798237        0.800933 -0.002697  \n",
       "15        0.798237        0.800933 -0.002697  \n",
       "51        0.798237        0.800156 -0.001919  \n",
       "25        0.797199        0.800156 -0.002956  \n",
       "27        0.797199        0.800156 -0.002956  \n",
       "24        0.797199        0.800156 -0.002956  \n",
       "36        0.797718        0.800156 -0.002438  \n",
       "48        0.798237        0.800156 -0.001919  \n",
       "50        0.798237        0.800156 -0.001919  \n",
       "49        0.798237        0.800156 -0.001919  \n",
       "37        0.797718        0.800156 -0.002438  \n",
       "26        0.797199        0.800156 -0.002956  \n",
       "39        0.798237        0.799378 -0.001141  \n",
       "38        0.798237        0.799378 -0.001141  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_models_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Results\n",
    "For random forest, row #15 was chosen for these reasons:\n",
    "* Matches or beats the accuracy of a larger forest \n",
    "\n",
    "* Has higher training accuracy\n",
    "\n",
    "* keeps the gap small (-0.002697)\n",
    "\n",
    "    * This model is best because it:\n",
    "        * Trains faster\n",
    "\n",
    "        * Uses fewer resources\n",
    "\n",
    "        * Performs just as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no hyperparameters worth tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lr = [] # empty list where results will go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(\n",
    "    random_state = 12345,\n",
    "    solver = 'liblinear' # best for small datesets, and classification\n",
    ")\n",
    "\n",
    "model_lr.fit(features_train, target_train)\n",
    "\n",
    "# model predictions\n",
    "train_preds = model_lr.predict(features_train)\n",
    "valid_preds = model_lr.predict(features_valid)\n",
    "\n",
    "# accuracy scores\n",
    "train_acc = accuracy_score(target_train, train_preds)\n",
    "valid_acc = accuracy_score(target_valid, valid_preds)\n",
    "\n",
    "results_lr.append({\n",
    "    'train_accuracy': train_acc,\n",
    "    'valid_accuracy': valid_acc,\n",
    "    'gap': train_acc - valid_acc\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning result into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lr_df = pd.DataFrame(results_lr)\n",
    "results_lr_df = results_lr_df.sort_values(by = 'valid_accuracy', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.710581</td>\n",
       "      <td>0.714619</td>\n",
       "      <td>-0.004038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_accuracy  valid_accuracy       gap\n",
       "0        0.710581        0.714619 -0.004038"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Results\n",
    "Logistic Regression model will not be selected for these reasons:\n",
    "\n",
    "* Both accuracies are below the 0.75 threshold for this project goal.\n",
    "\n",
    "* Logistic Regression may be too simple for this dataset, it assumes a linear relationship between features and output, which is not be enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection \n",
    "Random Forest was selected as the best model because it achieved:\n",
    "\n",
    "* The highest training and validation accuracy of all models\n",
    "\n",
    "* A minimal gap, suggesting strong generalization and low risk of overfitting\n",
    "\n",
    "Logistic Regression was not selected because its accuracy was below the required 0.75 threshold. While its generalization ability was solid (very small gap), its overall performance was not strong enough for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conducting final test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=5,\n",
       "                       n_estimators=20, random_state=12345)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = RandomForestClassifier(n_estimators = 20, max_depth = 3, min_samples_leaf = 5, min_samples_split = 5,\n",
    "random_state = 12345) # top_models_rf row 15\n",
    "\n",
    "best_model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.8009331259720062\n"
     ]
    }
   ],
   "source": [
    "final_preds = best_model.predict(features_valid)\n",
    "final_acc = accuracy_score(target_valid, final_preds)\n",
    "\n",
    "print('Final test accuracy:', final_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for model evaluation:\n",
    "* Used the exact hyperparameters from the best performing Random Forest model (row 15).\n",
    "\n",
    "* Trained it on the training set (features_train, target_train).\n",
    "\n",
    "* Tested it on the previously untouched test set (features_valid).\n",
    "\n",
    "* Got a final accuracy of 0.800933, which:\n",
    "\n",
    "    * Exceeds the 0.75 threshold\n",
    "\n",
    "    * Confirms the model generalizes well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a df that compares the prediction results to what the results should have actually been to find misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_df = features_valid.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_df['true_label'] = target_valid.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_df['predicted'] = final_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering misclassified rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = sanity_df[sanity_df['true_label'] != sanity_df['predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>51</td>\n",
       "      <td>356.79</td>\n",
       "      <td>67</td>\n",
       "      <td>11568.16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>141</td>\n",
       "      <td>1102.88</td>\n",
       "      <td>50</td>\n",
       "      <td>16951.74</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>44</td>\n",
       "      <td>15644.73</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>19</td>\n",
       "      <td>130.88</td>\n",
       "      <td>94</td>\n",
       "      <td>11510.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>60</td>\n",
       "      <td>419.02</td>\n",
       "      <td>21</td>\n",
       "      <td>11407.52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135</th>\n",
       "      <td>48</td>\n",
       "      <td>293.06</td>\n",
       "      <td>0</td>\n",
       "      <td>8139.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>40</td>\n",
       "      <td>227.89</td>\n",
       "      <td>38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>2</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>959.51</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>51</td>\n",
       "      <td>406.86</td>\n",
       "      <td>57</td>\n",
       "      <td>5336.82</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>49</td>\n",
       "      <td>359.24</td>\n",
       "      <td>0</td>\n",
       "      <td>6088.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      calls  minutes  messages   mb_used  true_label  predicted\n",
       "1763     51   356.79        67  11568.16           1          0\n",
       "2348    141  1102.88        50  16951.74           0          1\n",
       "1880      0     0.00        44  15644.73           1          0\n",
       "1269     19   130.88        94  11510.83           1          0\n",
       "591      60   419.02        21  11407.52           1          0\n",
       "3135     48   293.06         0   8139.80           1          0\n",
       "2363     40   227.89        38      0.00           1          0\n",
       "1065      2     2.00         0    959.51           1          0\n",
       "913      51   406.86        57   5336.82           1          0\n",
       "1421     49   359.24         0   6088.30           1          0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting average of each missclassified feature and subtracting it by each features_valid average to see:\n",
    "* Small differences (near 0), misclassified users are average, and hard to classify\n",
    "\n",
    "* Large differences, model may struggle with users who:\n",
    "\n",
    "    * Use more/less data than average\n",
    "\n",
    "    * Send way more/less messages\n",
    "\n",
    "    * Rarely call, or call a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    filtered = df.copy()\n",
    "    for col in df.select_dtypes(include = 'number'): # loop runs once for each column and filters numeric columns only\n",
    "        Q1 = df[col].quantile(0.25) # 25% of column values are below this\n",
    "        Q3 = df[col].quantile(0.75) # 75% of column values are above this\n",
    "        IQR = Q3 - Q1 # middle 50% range of values in column, less sensitive to outliers\n",
    "        lower = Q1 - 1.5 * IQR # lower outliers\n",
    "        upper = Q3 + 1.5 * IQR # upper outliers\n",
    "        filtered = filtered[(filtered[col] >= lower) & (filtered[col] <= upper)] # removing outlier values from each column\n",
    "    return filtered    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_means = remove_outliers(features_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_means = overall_means.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calls          60.377186\n",
       "minutes       418.198893\n",
       "messages       33.766861\n",
       "mb_used     16398.750433\n",
       "dtype: float64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassified averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_means = remove_outliers(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_means = misclassified_means.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calls            56.639810\n",
       "minutes         393.639194\n",
       "messages         31.990521\n",
       "mb_used       14158.315213\n",
       "true_label        1.000000\n",
       "predicted         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'Overall_avg': overall_means,\n",
    "    'misclassified_avg': misclassified_means,\n",
    "    'Difference': misclassified_means - overall_means\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_avg</th>\n",
       "      <th>misclassified_avg</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>calls</th>\n",
       "      <td>60.377186</td>\n",
       "      <td>56.639810</td>\n",
       "      <td>-3.737375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mb_used</th>\n",
       "      <td>16398.750433</td>\n",
       "      <td>14158.315213</td>\n",
       "      <td>-2240.435220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>messages</th>\n",
       "      <td>33.766861</td>\n",
       "      <td>31.990521</td>\n",
       "      <td>-1.776340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minutes</th>\n",
       "      <td>418.198893</td>\n",
       "      <td>393.639194</td>\n",
       "      <td>-24.559698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_label</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Overall_avg  misclassified_avg   Difference\n",
       "calls          60.377186          56.639810    -3.737375\n",
       "mb_used     16398.750433       14158.315213 -2240.435220\n",
       "messages       33.766861          31.990521    -1.776340\n",
       "minutes       418.198893         393.639194   -24.559698\n",
       "predicted            NaN           0.000000          NaN\n",
       "true_label           NaN           1.000000          NaN"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 256 entries, 1763 to 2845\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   calls       256 non-null    int64  \n",
      " 1   minutes     256 non-null    float64\n",
      " 2   messages    256 non-null    int64  \n",
      " 3   mb_used     256 non-null    float64\n",
      " 4   true_label  256 non-null    int64  \n",
      " 5   predicted   256 non-null    int64  \n",
      "dtypes: float64(2), int64(4)\n",
      "memory usage: 14.0 KB\n"
     ]
    }
   ],
   "source": [
    "misclassified.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(misclassified['true_label'] == 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check Results\n",
    "I analyzed 256 misclassified users, and 211 of them (82%) were labeled as Ultra but predicted as Smart. This suggests that these Ultra users exhibit behavioral patterns more typical of Smart users. Feature analysis confirmed that misclassified users had lower than average usage across all metrics, especially in data usage (2.2 GB less than average). These results indicate that the model is logically associating higher usage with the Ultra plan, and is likely misclassifying users who are subscribed to Ultra but do not take full advantage of it. This behavior aligns with real world expectations and does not suggest model bias or illogical errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions \n",
    "#### What models were used?\n",
    "* Three classification models were trained and evaluated:\n",
    "\n",
    "    * Decision Tree Classifier\n",
    "\n",
    "    * Random Forest Classifier\n",
    "\n",
    "    * Logistic Regression\n",
    "\n",
    "#### How they performed...\n",
    "\n",
    "* Decision Tree (Row 13)\n",
    "\n",
    "    * Training accuracy: 0.790975\n",
    "\n",
    "    * Validation accuracy: 0.788491\n",
    "\n",
    "    * Gap: 0.002484\n",
    "\n",
    "* Random Forest (Row 15)\n",
    "\n",
    "    * Training accuracy: 0.798237\n",
    "\n",
    "    * Validation accuracy: 0.800933\n",
    "\n",
    "    * Gap: -0.002697 (slight positive generalization)\n",
    "\n",
    "* Logistic Regression\n",
    "\n",
    "    * Training accuracy: 0.710581\n",
    "\n",
    "    * Validation accuracy: 0.714619\n",
    "\n",
    "    * Gap: -0.004038\n",
    "\n",
    "#### What tuning steps were effective?\n",
    "\n",
    "* Decision Tree:\n",
    "\n",
    "    * Tuned max_depth (2-6), min_samples_leaf (1, 5, 10), min_samples_split (2, 5, 10)\n",
    "\n",
    "    * Row 13 was chosen for its balance in leaf/split values (5 each) and low overfitting risk with no accuracy loss.\n",
    "\n",
    "* Random Forest:\n",
    "\n",
    "    * Tuned n_estimators (10 to 50, step 10), max_depth (3, 5, 7), min_samples_leaf (1, 5), min_samples_split (2, 5)\n",
    "\n",
    "    * Row 15 was selected for strong accuracy and generalization, outperforming deeper or more complex forests.\n",
    "\n",
    "* Logistic Regression:\n",
    "\n",
    "    * No tuning was required; default parameters used with liblinear solver.\n",
    "\n",
    "    * Results were below the 0.75 threshold, so the model was not selected.\n",
    "\n",
    "#### Final test accuracy\n",
    "\n",
    "The final test was performed using the best Random Forest model (Row 15):\n",
    "\n",
    "RandomForestClassifier(\n",
    "    n_estimators = 20,\n",
    "    max_depth = 3,\n",
    "    min_samples_leaf = 5,\n",
    "    min_samples_split = 5,\n",
    "    random_state = 12345\n",
    ")\n",
    "\n",
    "Final test accuracy: 0.800933\n",
    "\n",
    "This result exceeded the project goal of 0.75 and validated the model's ability to generalize beyond the training and validation sets.\n",
    "\n",
    "## Summary\n",
    "\n",
    "* The Random Forest model proved to be the most effective classifier due to its:\n",
    "\n",
    "    * High accuracy\n",
    "\n",
    "    * Generalization ability (low gap)\n",
    "\n",
    "    * Moderate complexity\n",
    "\n",
    "The Decision Tree also performed well, but Random Forest offered a stronger performance overall. Logistic Regression did not meet the required accuracy threshold, likely due to the complexity of the dataset exceeding what linear modeling can capture.\n",
    "\n",
    "Final model results and the detailed sanity check confirmed the model's reliability and its logical behavior in edge cases."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 301,
    "start_time": "2025-03-31T16:53:36.263Z"
   },
   {
    "duration": 467,
    "start_time": "2025-03-31T16:55:14.035Z"
   },
   {
    "duration": 8,
    "start_time": "2025-03-31T16:55:38.816Z"
   },
   {
    "duration": 215,
    "start_time": "2025-03-31T16:56:08.603Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-31T16:56:20.813Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-31T16:57:16.173Z"
   },
   {
    "duration": 19,
    "start_time": "2025-03-31T18:03:42.100Z"
   },
   {
    "duration": 12,
    "start_time": "2025-03-31T18:03:53.660Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-31T18:06:11.697Z"
   },
   {
    "duration": 21,
    "start_time": "2025-03-31T18:07:01.377Z"
   },
   {
    "duration": 340,
    "start_time": "2025-03-31T18:17:58.792Z"
   },
   {
    "duration": 7,
    "start_time": "2025-03-31T18:18:38.445Z"
   },
   {
    "duration": 12,
    "start_time": "2025-03-31T18:38:03.010Z"
   },
   {
    "duration": 6,
    "start_time": "2025-03-31T18:38:42.753Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-31T18:39:36.229Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-31T18:39:51.799Z"
   },
   {
    "duration": 10,
    "start_time": "2025-03-31T18:40:06.287Z"
   },
   {
    "duration": 6,
    "start_time": "2025-03-31T18:40:33.853Z"
   },
   {
    "duration": 12,
    "start_time": "2025-03-31T18:40:36.279Z"
   },
   {
    "duration": 367,
    "start_time": "2025-03-31T18:41:02.290Z"
   },
   {
    "duration": 483,
    "start_time": "2025-03-31T18:41:02.659Z"
   },
   {
    "duration": 7,
    "start_time": "2025-03-31T18:41:03.144Z"
   },
   {
    "duration": 2,
    "start_time": "2025-03-31T18:41:03.154Z"
   },
   {
    "duration": 16,
    "start_time": "2025-03-31T18:41:03.157Z"
   },
   {
    "duration": 10,
    "start_time": "2025-03-31T18:41:03.174Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-31T18:41:03.186Z"
   },
   {
    "duration": 23,
    "start_time": "2025-03-31T18:41:03.192Z"
   },
   {
    "duration": 7,
    "start_time": "2025-03-31T18:41:03.217Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-31T18:41:03.227Z"
   },
   {
    "duration": 39,
    "start_time": "2025-03-31T18:41:03.233Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-31T18:41:03.274Z"
   },
   {
    "duration": 9,
    "start_time": "2025-03-31T18:53:22.007Z"
   },
   {
    "duration": 4,
    "start_time": "2025-03-31T19:00:43.674Z"
   },
   {
    "duration": 3,
    "start_time": "2025-03-31T19:01:14.374Z"
   },
   {
    "duration": 217,
    "start_time": "2025-03-31T19:04:00.266Z"
   },
   {
    "duration": 5,
    "start_time": "2025-03-31T19:04:29.508Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T16:30:23.650Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T16:30:45.772Z"
   },
   {
    "duration": 155,
    "start_time": "2025-04-01T16:39:22.953Z"
   },
   {
    "duration": 295,
    "start_time": "2025-04-01T16:39:36.961Z"
   },
   {
    "duration": 480,
    "start_time": "2025-04-01T16:39:37.258Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-01T16:39:37.740Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T16:39:37.751Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T16:39:37.757Z"
   },
   {
    "duration": 20,
    "start_time": "2025-04-01T16:39:37.763Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T16:39:37.784Z"
   },
   {
    "duration": 30,
    "start_time": "2025-04-01T16:39:37.789Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-01T16:39:37.821Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T16:39:37.828Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-01T16:39:37.834Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T16:39:37.844Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T16:39:37.850Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-01T16:39:37.857Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T16:39:37.861Z"
   },
   {
    "duration": 41,
    "start_time": "2025-04-01T16:39:37.867Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T16:39:37.910Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T16:44:05.355Z"
   },
   {
    "duration": 256,
    "start_time": "2025-04-01T16:50:34.517Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T16:52:08.842Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T16:52:52.872Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-01T16:53:57.236Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-01T17:06:56.662Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T17:10:09.249Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-01T17:10:17.197Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T17:11:06.222Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T17:11:06.938Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T17:11:50.347Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T17:11:50.763Z"
   },
   {
    "duration": 267,
    "start_time": "2025-04-01T17:12:02.097Z"
   },
   {
    "duration": 458,
    "start_time": "2025-04-01T17:12:02.367Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-01T17:12:02.826Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:12:02.836Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-01T17:12:02.842Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:12:02.845Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-01T17:12:02.850Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:12:02.860Z"
   },
   {
    "duration": 47,
    "start_time": "2025-04-01T17:12:02.865Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-01T17:12:02.914Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:12:02.921Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-01T17:12:02.926Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T17:12:02.935Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:12:02.942Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T17:12:02.946Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T17:12:02.952Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:12:02.959Z"
   },
   {
    "duration": 301,
    "start_time": "2025-04-01T17:12:02.964Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:12:03.266Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:12:03.271Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-01T17:12:03.275Z"
   },
   {
    "duration": 23,
    "start_time": "2025-04-01T17:12:03.288Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T17:12:03.313Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-01T17:20:54.783Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:27:28.175Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T17:28:02.455Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-01T17:28:03.082Z"
   },
   {
    "duration": 281,
    "start_time": "2025-04-01T17:28:14.068Z"
   },
   {
    "duration": 479,
    "start_time": "2025-04-01T17:28:14.352Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-01T17:28:14.833Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:28:14.842Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-01T17:28:14.847Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T17:28:14.851Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-01T17:28:14.856Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:28:14.867Z"
   },
   {
    "duration": 39,
    "start_time": "2025-04-01T17:28:14.872Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-01T17:28:14.912Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:28:14.920Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-01T17:28:14.925Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T17:28:14.934Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:28:14.943Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-01T17:28:14.948Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T17:28:14.952Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:28:14.958Z"
   },
   {
    "duration": 261,
    "start_time": "2025-04-01T17:28:15.009Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:28:15.271Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:28:15.276Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-01T17:28:15.281Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-01T17:28:15.309Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T17:28:15.313Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T17:28:15.319Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T17:30:14.009Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T17:30:36.756Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T17:31:50.635Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T17:32:26.272Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-01T17:32:56.090Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-01T17:32:56.994Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T18:19:11.154Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T18:19:43.017Z"
   },
   {
    "duration": 3837,
    "start_time": "2025-04-01T18:37:46.294Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-01T18:39:09.371Z"
   },
   {
    "duration": 13,
    "start_time": "2025-04-01T18:39:17.265Z"
   },
   {
    "duration": 3896,
    "start_time": "2025-04-01T18:40:30.769Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T18:40:34.668Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-01T18:40:34.675Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-01T18:41:19.013Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-01T18:41:29.605Z"
   },
   {
    "duration": 341,
    "start_time": "2025-04-01T18:41:43.944Z"
   },
   {
    "duration": 546,
    "start_time": "2025-04-01T18:41:44.287Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-01T18:41:44.837Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-01T18:41:44.849Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T18:41:44.854Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T18:41:44.858Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-01T18:41:44.863Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T18:41:44.876Z"
   },
   {
    "duration": 50,
    "start_time": "2025-04-01T18:41:44.881Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-01T18:41:44.933Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T18:41:44.942Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-01T18:41:44.949Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T18:41:44.960Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T18:41:44.968Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T18:41:44.972Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-01T18:41:45.016Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T18:41:45.025Z"
   },
   {
    "duration": 300,
    "start_time": "2025-04-01T18:41:45.030Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-01T18:41:45.332Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-01T18:41:45.338Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-01T18:41:45.346Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-01T18:41:45.359Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-01T18:41:45.367Z"
   },
   {
    "duration": 32,
    "start_time": "2025-04-01T18:41:45.378Z"
   },
   {
    "duration": 3831,
    "start_time": "2025-04-01T18:41:45.412Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-01T18:41:49.245Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T18:41:49.251Z"
   },
   {
    "duration": 17,
    "start_time": "2025-04-01T18:41:49.257Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-01T18:49:44.529Z"
   },
   {
    "duration": 14,
    "start_time": "2025-04-01T18:49:51.726Z"
   },
   {
    "duration": 291,
    "start_time": "2025-04-02T00:11:41.774Z"
   },
   {
    "duration": 493,
    "start_time": "2025-04-02T00:11:42.068Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-02T00:11:42.562Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T00:11:42.573Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T00:11:42.588Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T00:11:42.592Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-02T00:11:42.596Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T00:11:42.613Z"
   },
   {
    "duration": 19,
    "start_time": "2025-04-02T00:11:42.618Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-02T00:11:42.641Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T00:11:42.648Z"
   },
   {
    "duration": 32,
    "start_time": "2025-04-02T00:11:42.654Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-02T00:11:42.688Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T00:11:42.695Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T00:11:42.699Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-02T00:11:42.704Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T00:11:42.711Z"
   },
   {
    "duration": 286,
    "start_time": "2025-04-02T00:11:42.714Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T00:11:43.002Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T00:11:43.006Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-02T00:11:43.011Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-02T00:11:43.023Z"
   },
   {
    "duration": 20,
    "start_time": "2025-04-02T00:11:43.029Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T00:11:43.051Z"
   },
   {
    "duration": 3610,
    "start_time": "2025-04-02T00:11:43.055Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T00:11:46.667Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-02T00:11:46.671Z"
   },
   {
    "duration": 12,
    "start_time": "2025-04-02T00:11:46.690Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T00:11:46.703Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-02T00:11:46.709Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-02T01:50:06.471Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-02T01:50:29.995Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T02:08:03.761Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T02:08:18.707Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T02:09:29.119Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T02:09:54.273Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T02:37:31.179Z"
   },
   {
    "duration": 290,
    "start_time": "2025-04-02T03:05:38.803Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-02T03:07:01.913Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T03:08:54.365Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-02T03:09:00.039Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T03:42:04.889Z"
   },
   {
    "duration": 36,
    "start_time": "2025-04-02T03:43:56.844Z"
   },
   {
    "duration": 14,
    "start_time": "2025-04-02T03:45:56.062Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-02T03:46:04.312Z"
   },
   {
    "duration": 292,
    "start_time": "2025-04-02T16:12:17.101Z"
   },
   {
    "duration": 477,
    "start_time": "2025-04-02T16:12:17.395Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-02T16:12:17.874Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T16:12:17.884Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T16:12:17.890Z"
   },
   {
    "duration": 18,
    "start_time": "2025-04-02T16:12:17.894Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T16:12:17.913Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T16:12:17.918Z"
   },
   {
    "duration": 17,
    "start_time": "2025-04-02T16:12:17.922Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T16:12:17.940Z"
   },
   {
    "duration": 19,
    "start_time": "2025-04-02T16:12:17.944Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-02T16:12:17.966Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-02T16:12:17.976Z"
   },
   {
    "duration": 40,
    "start_time": "2025-04-02T16:12:17.984Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-02T16:12:18.026Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-02T16:12:18.035Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T16:12:18.044Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-02T16:12:18.049Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T16:12:18.057Z"
   },
   {
    "duration": 291,
    "start_time": "2025-04-02T16:12:18.061Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T16:12:18.353Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T16:12:18.358Z"
   },
   {
    "duration": 13,
    "start_time": "2025-04-02T16:12:18.363Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T16:12:18.377Z"
   },
   {
    "duration": 32,
    "start_time": "2025-04-02T16:12:18.383Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T16:12:18.417Z"
   },
   {
    "duration": 3591,
    "start_time": "2025-04-02T16:12:18.421Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T16:12:22.013Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T16:12:22.018Z"
   },
   {
    "duration": 12,
    "start_time": "2025-04-02T16:12:22.024Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T16:12:22.038Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-02T16:12:22.043Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T16:12:22.053Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-02T16:12:22.057Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T16:12:22.069Z"
   },
   {
    "duration": 39,
    "start_time": "2025-04-02T16:12:22.074Z"
   },
   {
    "duration": 33,
    "start_time": "2025-04-02T16:12:22.114Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-02T16:12:22.148Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T16:33:00.872Z"
   },
   {
    "duration": 214,
    "start_time": "2025-04-02T16:34:11.514Z"
   },
   {
    "duration": 14,
    "start_time": "2025-04-02T16:34:28.425Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T16:35:07.340Z"
   },
   {
    "duration": 13,
    "start_time": "2025-04-02T16:35:08.545Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T16:35:16.092Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T16:35:56.837Z"
   },
   {
    "duration": 13,
    "start_time": "2025-04-02T16:38:30.716Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T16:40:30.165Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-02T16:49:39.069Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-02T16:57:23.915Z"
   },
   {
    "duration": 13,
    "start_time": "2025-04-02T16:57:46.101Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T17:40:50.851Z"
   },
   {
    "duration": 24,
    "start_time": "2025-04-02T17:42:23.630Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T17:44:28.862Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-02T17:44:29.456Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-02T17:45:48.161Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T17:46:26.343Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-02T17:46:39.452Z"
   },
   {
    "duration": 13,
    "start_time": "2025-04-02T17:47:22.637Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T17:47:45.904Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T17:47:56.176Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T17:52:40.066Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-02T17:52:43.841Z"
   },
   {
    "duration": 13,
    "start_time": "2025-04-02T17:55:57.723Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-02T17:56:02.984Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T17:56:43.788Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-02T17:56:56.154Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:22.336Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:22.340Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:22.345Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:22.349Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:22.354Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:22.358Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T18:43:22.363Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:22.367Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-02T18:43:22.372Z"
   },
   {
    "duration": 50,
    "start_time": "2025-04-02T18:43:22.382Z"
   },
   {
    "duration": 19,
    "start_time": "2025-04-02T18:43:22.434Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-02T18:43:22.455Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:43:22.465Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-02T18:43:22.471Z"
   },
   {
    "duration": 49,
    "start_time": "2025-04-02T18:43:22.482Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:43:22.534Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:22.540Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-02T18:43:22.545Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:22.553Z"
   },
   {
    "duration": 304,
    "start_time": "2025-04-02T18:43:22.558Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-02T18:43:22.865Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:43:22.873Z"
   },
   {
    "duration": 13,
    "start_time": "2025-04-02T18:43:22.879Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:43:22.894Z"
   },
   {
    "duration": 34,
    "start_time": "2025-04-02T18:43:22.900Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T18:43:22.938Z"
   },
   {
    "duration": 3684,
    "start_time": "2025-04-02T18:43:22.942Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-02T18:43:26.628Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:26.636Z"
   },
   {
    "duration": 17,
    "start_time": "2025-04-02T18:43:26.641Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:26.661Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-02T18:43:26.666Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:26.679Z"
   },
   {
    "duration": 50,
    "start_time": "2025-04-02T18:43:26.684Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:43:26.738Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-02T18:43:26.745Z"
   },
   {
    "duration": 35,
    "start_time": "2025-04-02T18:43:26.755Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-02T18:43:26.792Z"
   },
   {
    "duration": 29,
    "start_time": "2025-04-02T18:43:26.803Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:43:26.834Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:26.840Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:43:26.845Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-02T18:43:26.851Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:43:26.863Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-02T18:43:26.869Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:43:26.881Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-02T18:43:26.930Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-02T18:43:26.937Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:43:26.954Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-02T18:43:26.960Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-02T18:43:26.967Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-02T18:43:26.978Z"
   },
   {
    "duration": 45,
    "start_time": "2025-04-02T18:43:26.988Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:43:27.036Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T18:51:32.127Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:32.131Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:51:32.137Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:51:32.142Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T18:51:32.147Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:32.150Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:32.155Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:51:32.160Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-02T18:51:32.164Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:51:32.175Z"
   },
   {
    "duration": 60,
    "start_time": "2025-04-02T18:51:32.180Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-02T18:51:32.241Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:32.248Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-02T18:51:32.256Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:32.264Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:32.269Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:51:32.275Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-02T18:51:32.279Z"
   },
   {
    "duration": 45,
    "start_time": "2025-04-02T18:51:32.286Z"
   },
   {
    "duration": 260,
    "start_time": "2025-04-02T18:51:32.332Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:51:32.593Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:32.598Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-02T18:51:32.603Z"
   },
   {
    "duration": 17,
    "start_time": "2025-04-02T18:51:32.616Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-02T18:51:32.634Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-02T18:51:32.644Z"
   },
   {
    "duration": 3594,
    "start_time": "2025-04-02T18:51:32.648Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:36.245Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:51:36.251Z"
   },
   {
    "duration": 12,
    "start_time": "2025-04-02T18:51:36.257Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:51:36.271Z"
   },
   {
    "duration": 9,
    "start_time": "2025-04-02T18:51:36.276Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:51:36.286Z"
   },
   {
    "duration": 41,
    "start_time": "2025-04-02T18:51:36.290Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:36.332Z"
   },
   {
    "duration": 12,
    "start_time": "2025-04-02T18:51:36.337Z"
   },
   {
    "duration": 43,
    "start_time": "2025-04-02T18:51:36.351Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-02T18:51:36.396Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:51:36.404Z"
   },
   {
    "duration": 24,
    "start_time": "2025-04-02T18:51:36.409Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:51:36.438Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:36.442Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-02T18:51:36.448Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:36.458Z"
   },
   {
    "duration": 10,
    "start_time": "2025-04-02T18:51:36.464Z"
   },
   {
    "duration": 3,
    "start_time": "2025-04-02T18:51:36.475Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-02T18:51:36.479Z"
   },
   {
    "duration": 46,
    "start_time": "2025-04-02T18:51:36.487Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:36.534Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:36.540Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-02T18:51:36.545Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-02T18:51:36.550Z"
   },
   {
    "duration": 8,
    "start_time": "2025-04-02T18:51:36.559Z"
   },
   {
    "duration": 5,
    "start_time": "2025-04-02T18:51:36.569Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
